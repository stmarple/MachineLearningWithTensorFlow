# -*- coding: utf-8 -*-
"""FFNN_MNST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aiqZc6KtceFijExgmGAsrlujxmnviY4P

# MNIST (aka Modified National Institute of Standards and Technology Database) 
is a large database of handwritten digits used for training various image processing systems.
https://en.wikipedia.org/wiki/MNIST_database
"""

from __future__ import absolute_import, division, print_function, unicode_literals
import tensorflow as tf

"""### **Load and Prepare the MNIST Dataset**"""

mnist = tf.keras.datasets.mnist

(x_train, y_train), (x_test, y_test) = mnist.load_data()
x_train, x_test = x_train / 255.0, x_test / 255.0

"""### **Build the `tf.keras.Sequential` model by stacking layers**"""

modelA = tf.keras.models.Sequential([
  tf.keras.layers.Flatten(input_shape=(28, 28)),
  tf.keras.layers.Dense(256, activation='relu'),
  tf.keras.layers.Dropout(0.2),
  tf.keras.layers.Dense(10, activation='softmax')
])

modelA.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])

"""### **Train and evaluate the model**
Before we begin, we should go over a few important **hyperparameters**:

1. epoch, which is the total training sequences
2. batch_size, which is the training batch size
3. display_freq, which is the frequency of results displaying
4. learning_rate, which is the initial optimization learning rate

see https://www.easy-tensorflow.com/tf-tutorials/neural-networks/two-layer-neural-network?view=article&id=124:two-layer-neural-network for more information.
"""

historyA = modelA.fit(x_train, y_train, epochs=5)
loss, accuracy  = modelA.evaluate(x_test,  y_test, verbose=2)

"""**Shape**

From here, we can extrapolate that the training data set has 60,000 samples of images with the dimensions 28 X 28.  We also see that the test sample is composed of 10,000 samples of images of the same dimensions.
"""

print('Training data shape: ', x_train.shape)
print('Test data shape: ', x_test.shape)

"""**Flatten Images**

The image vector size is the square units of each image.  In this case, 28 * 28, or 784.   
As we saw above, x_train = [60000, 28, 28], so x_train[0] is equal to our sample size, or 60,000.    
As the section says, we are trying to flatten this image.  Rather than being 3 dimensional 60k X 28 X 28,
we are flattening the image population down to area.  So we will be down to (sample size, area) for each: training set and test set.
"""

img_vector_size = 28**2
x_train = x_train.reshape(x_train.shape[0], img_vector_size)
x_test = x_test.reshape(x_test.shape[0], img_vector_size)

import matplotlib.pyplot as plt

plt.plot(historyA.history['acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['training', 'validation'], loc='best')
plt.show()

print(f'Test loss: {loss:.3}')
print(f'Test accuracy: {accuracy:.3}')

"""*** Now that looks like some good accuracy, with small amounts of test loss! ***

Let's see what it looks like when applied to a hand-written digit...
"""

from random import randint,sample

def test_model(img):
  plt.figure
  plt.imshow(x_test[img].reshape(28,28),cmap='Greys')
  pred = modelA.predict(x_test[img].reshape(1, 28, 28))
  return pred.argmax()

img = randint(1,5000)
plt.imshow(x_test[img].reshape(28,28),cmap='Greys')
pred = modelA.predict(x_test[img].reshape(1, 28, 28))
print(pred.argmax())

img = 1500
plt.imshow(x_test[img].reshape(28,28),cmap='Greys')
pred = modelA.predict(x_test[img].reshape(1, 28, 28))
print(pred.argmax())

"""*** Looks like this classifies these digits pretty well! ***
# In this next section... 
We will discuss alternate methods of achieving similiar success.  We will look to add nodes and layers, we will choose different optimizers and activation functions and see how that changes the outcome.  We will also go step-by-step on the models work, what each optimizer and activation functions are [that we may use in a problem like this], as well as how the steps are calculated to get a decent understanding on how the model works behind the scenes to enable us to build a foundation for understanding machine learning.

---

# Classification of MNIST Digits
https://medium.com/tebs-lab/how-to-classify-mnist-digits-with-different-neural-network-architectures-39c75a0f03e3 

Before we get started, there a just few things we should talk about:  
* **Training data** is the data that our model will be learning from
* **Test data** is the data that is kept a secret until after the model has been trained, and then evaluated against
* A **loss function** is used to quantify how accurate our model's predictions are. 
  These are the amount of error involved.  Some typical loss functions are:
  - Mean Squared Error (MSE)
  - Mean Absolute Error (MAE)
  - Mean Bias Error (MBE)
  - Cross Entropy Loss / Negative Log Likelihood
  
  Many of these I have seen in my studies in Statistics
* An **optimization algorithm** controls the weights, that are adjusted during training.



For more information on loss functions, you can find it here:

https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23

### Calculating Cross Entropy Loss
Since Cross Entropy Loss is the most common setting for classification problems, we will take a closer look at its calculation.
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import log_loss
import numpy as np

"""# [Categorical] Cross Entropy:
## H(p, q) = E<sub>p</sub> [ -log q(x) ] = -$\sum_{i}^C$ p<sub>i</sub>(x) * Log q<sub>i</sub>(x)
Where:
* p is the true probability, given the distribution
* q is the predicted value of the current model
* C is the number of classes, which in our digit classification problem, would be 10
* The above formula is how we find it in many websites.  However, the log is actually base e, not implied base 10, so it should actually be log<sub>e</sub> , or LN. So from now on, in this document, LN is how I'll be writing it; to avoid further confusion.

### _So, let's try to understand this a bit_
 
Say we have an image of digit 2.
Each image can only be entirely one number on a scale of 0 to 9. 

#### Therefore our vector would look like this: 

> p<sub>1</sub> = [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]

There is 0 probability of being any other number but the one the image is supposed to be.

### Now, let's say we have a machine learning model that classifies the image with probabilities of:

> q<sub>1</sub> = [0.0, 0.0, 0.65, 0.15, 0.0, 0.0, 0.0, 0.20, 0.0, 0.0]

Cross entropy can be calculated on this single image as:

## -$\sum_{i}^C$ p<sub>i</sub>(x) * LN q<sub>i</sub>(x)

= - (0 * LN(0) + 0 * LN(0) + 1 * LN(0.65) + 0 * LN(0.15) + 0 * LN(0) + 0 * LN(0.20) + 0 * LN(0) + 0 * LN(0) + 0 * LN(0) + 0 * LN(0))

= - (LN(0.65))

= 0.43078

### Now, lets say the model was able to get better and classifies the probabilies of this image as:
q<sub>1</sub> = [0.0, 0.0, 0.95, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

So, 

## -$\sum_{i}^C$ p<sub>i</sub>(x) * LN q<sub>i</sub>(x) 

= - (0 * LN(0) + 0 * LN(0) + 1 * LN(0.95) + 0 * LN(0.05) + 0 * LN(0) + 0 * LN(0.20) + 0 * LN(0) + 0 * LN(0) + 0 * LN(0) + 0 * LN(0))

= - (LN(0.95))

= 0.05129

**This is much better!**

#### If you would like to get more information on this, see: 
* https://en.wikipedia.org/wiki/Cross_entropy 
* https://towardsdatascience.com/understanding-binary-cross-entropy-log-loss-a-visual-explanation-a3ac6025181a
* https://medium.com/activating-robotic-minds/demystifying-cross-entropy-e80e3ad54a8
* A Questionable StackOverflow CE function: https://stackoverflow.com/questions/49473587/how-to-compute-log-loss-same-as-cross-entropy
"""

def cross_entropy(x, y):
    x = [element if element > 0 else 1 for element in x] # replace prediction probabilities of 0 to prevent LN(0), which is undefined.  Instead replace LN(1) = 0, which cancels out the term anyway
    print(x)
    pred = np.array(x)
    targ = np.array(y)
    
    N = pred.shape[0] # A stack overflow divided by this number, which seems to conflict with my research (see above)
    return -np.sum(targ*np.log(pred))

predictions1 = [0.0, 0.0, 0.65, 0.15, 0.0, 0.20, 0.0, 0.0, 0.0, 0.0]
targets1     = [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

cross_entropy(predictions1, targets1)

predictions2 =  [0.0, 0.0, 0.95, 0.05, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]
targets2     =  [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]

cross_entropy(predictions2, targets2)

"""# Example 2: Building a Highly Accurate Image Classifification Model Step-by-Step
## Create a Vector
"""

import keras
from keras.datasets import mnist

# Setup train and test splits
(x_train, y_train), (x_test, y_test) = mnist.load_data()
print('Training label shape: '   , y_train.shape) # 60000 numbers (all 0-9)
print('First 5 training labels: ', y_train[:5]) # [5, 0, 4, 1, 9]

# Convert to vectors using the to_categorical function
num_classes = 10
y_train = keras.utils.to_categorical(y_train, num_classes)
y_test = keras.utils.to_categorical(y_test, num_classes)
print('First 5 training labels as one-hot encoded vectors:\n', y_train[:5])

"""The **number of classes** represents the number of unique digits we have.  This will be the number of categories we can split them up into.  This number represents the number of output nodes or probabilities.

### For fully connected neural networks, there are three essential questions that define the networkâ€™s architecture:
1. How many layers are there?
2. How many nodes are there in each of those layers?
3. What transfer/activation function is used at each of those layers?

### There are 2 factors that contribute to the performance of a neural network: 
* The loss function  
* The optimization algorithm used

Per the article mentioned above, the author selects:
* A *common loss function*: **the categorical cross entropy** and 
* One of the simpler *optimization alogorithms*: **the stocastic gradient descent (SGD)**

**What is cross entropy loss?**  It is a log loss that measures the performance of a classification model whose output is based on a probability between 0 and 1.

More information on loss functions and optimiizers can be found here: https://ml-cheatsheet.readthedocs.io/en/latest/optimizers.html#sgd

## Creating First Model
Dense layers are "fully connected" layers
Documentation: https://keras.io/models/sequential/

* "The sequential model is a linear stack of layers" (https://keras.io/getting-started/sequential-model-guide/)

* The input layer requires the special input_shape parameter which should match the shape of our training data.
* The image_size is a created by flattening an image to 28 X 28 or 28<sup>2</sup>  = 784
* This model has a single hidden layer, that has 32 nodes, or 32 biases using the sigmoid activation function
* And, since there are 784 square units, on 1 layer, that has 32 nodes, there are 784 x 1 x 32 = 25,088 weights, where weights represent the number of pixels
* Therefore, there are 25,088 + 32 biases = 25,120 parameters
* There are 32 x 10, or 320 weights from hidden layer to output layer.
* Each of the 10 nodes adds a single bias >> 25,120 par + 320 weights + 10 nodes = 25,450 total parameters
"""

from keras.layers import Dense, Flatten 
from keras.models import Sequential 

image_size = 784 # 28*28
num_classes = 10 # ten unique digits

model = Sequential()
model.add(Flatten(input_shape=(28, 28)))
model.add(Dense(units=32, activation='sigmoid', input_shape=(image_size,)))
model.add(Dense(units=num_classes, activation='softmax'))
model.summary()

"""### This model contains a special activation function: the softmax.
What makes it special is that it normalizes the values from the 10 output nodes in such a way that:
* all the values are between 0 and 1
* the sum of all 10 values = 1

To put it another way, Softmax "calculates the probabilities distribution of the event over 'n' different events".  In this case, there are 10 possible probabilities, with the largest as the prediction vector.  These probabilities "will be helpful when determining the target class for the given inputs".

More information on activation functions can be found here:
https://ml-cheatsheet.readthedocs.io/en/latest/activation_functions.html

## Train and Evaluate The Model
"""

model.compile(optimizer="sgd", loss='categorical_crossentropy', metrics=['accuracy'])

history = model.fit(x_train, y_train, batch_size=128, epochs=5, validation_split=0.05)
loss, accuracy  = model.evaluate(x_test, y_test, verbose=False)

"""### Graph That"""

import matplotlib.pyplot as plt

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['training', 'validation'], loc='best')
plt.show()

print(f'Test loss: {loss:.3}')
print(f'Test accuracy: {accuracy:.3}')

"""## Create, Test, Evaluate, and Graph Again
This time, rather than having a single layer with only 32 nodes, let's increase that to 128 nodes
"""

image_size = 784 # 28*28
num_classes = 10 # ten unique digits

model = Sequential()
model.add(Flatten(input_shape=(28, 28)))
model.add(Dense(units=128, activation='sigmoid', input_shape=(image_size,))) # increase from 32 nodes to 128 **
model.add(Dense(units=num_classes, activation='softmax'))
model.summary()

model.compile(optimizer="sgd", loss='categorical_crossentropy', metrics=['accuracy'])
history = model.fit(x_train, y_train, batch_size=128, epochs=5, validation_split=0.1)
loss, accuracy  = model.evaluate(x_test, y_test, verbose=False)

plt.plot(history.history['acc'])
plt.plot(history.history['val_acc'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['training', 'validation'], loc='best')
plt.show()

print(f'Test loss: {loss:.3}')
print(f'Test accuracy: {accuracy:.3}')

"""It looks like the training model has gotten much better.  However, there is still much to be done about the test loss of more than 32%.

## Let's see what this model can predict...
"""

img = 1500
plt.imshow(x_test[img].reshape(28,28),cmap='Greys')
pred = model.predict(x_test[img].reshape(1, 28, 28))
print(pred.argmax())

"""*** Looks like we still have some work to do! ***

This image is obviously not a 1.  We will look to increase accuracy in the next section.

## Network Depth and Layer Width
https://medium.com/tebs-lab/how-to-classify-mnist-digits-with-different-neural-network-architectures-39c75a0f03e3 **continues...**

### In this section, we will be adding layers and determining whether or not there is a positive impact on our model
"""

image_size = 784 # 28*28
num_classes = 10 # ten unique digits

def create_depth(layer_sizes):
    model = Sequential()
    model.add(Flatten(input_shape=(28, 28)))
    model.add(Dense(units=128, activation='sigmoid', input_shape=(image_size,))) # increase from 32 nodes to 128 **
    
    for layer_size in layer_sizes[1:]:
      model.add(Dense(units=num_classes, activation='sigmoid'))
    
    model.add(Dense(units=num_classes, activation='softmax'))
    return model
    
def eval(model, batch_size=128, epochs=5):
    model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])
    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=False)
    loss, accuracy  = model.evaluate(x_test, y_test, verbose=False)

    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['training', 'validation'], loc='best')
    plt.show()

    print(f'Test loss: {loss:.3}')
    print(f'Test accuracy: {accuracy:.3}')

"""### As we can see in the next cells, added hidden layers with sigmoid activation functions and 32 nodes, doesn't seem to help us on the quest to best accuracy.  We may need to consider tweaking for at least some positive change"""

for layers in range(1,4):
    mod = create_depth([32] * layers)
    if layers == 1:
        print(f'Model with{layers: 1} hidden layer')
    else:
        print(f'Model with{layers: 1} hidden layers')
    eval(mod)
    print()

"""As you can see, the current model is using:
* SGD, or stochastic gradient descent, is a system or process that selects a few samples at random for each iteration, rather than selecting the entire dataset.  SGD uses a single sample, ie a batch of 1, to perform each iteration.  The sample is then shuffled and selected for the iteration.  The following is the calculation for the SGD algorithm: 

> for i in range( m ):
> > $\theta$<sub>j</sub> = $\theta$<sub>j</sub> - $\alpha$ * ( $\hat{y}$<sup>i</sup> - y<sup>i</sup> ) * x<sub>j</sub><sup>i</sup> 

> Although I have not been able to find an understandable breakdown of this equation.  It appears to resemble the formula of a line: y - y<sub>1</sub> = m(x - x<sub>1</sub>)
* loss = categorical_crossentropy
* The create_depth function will create a multilayer perceptron with a selected array of sizes, each with 32 nodes
* As we can see from the results, as soon as an additional layer was added, our model immediately became ***overfit***.*

### Now, let's consider training the same 32 nodes for 50 epochs instead of 5...
"""

for layers in range(1,4):
    mod = create_depth([32] * layers)
    if layers == 1:
        print(f'Model with{layers: 1} hidden layer')
    else:
        print(f'Model with{layers: 1} hidden layers')
    eval(mod, 128, 50)
    print()

"""### It still looks like 1 hidden layer is still the best, with an improved accuracy of nearly 96%.

## Now, we will take a closer look at what happens as we increase / decrease numbers of nodes (or pixels)
"""

for nodes in [32, 128, 512, 2048]:
    mod = create_depth([nodes])
    print(f'Model with{nodes: 1} nodes')
    eval(mod)
    print()

"""### As we can see, the number of nodes in this example failed to really make an impact on our model

## Now, let's try adding larger layers to the model
In other words, we will increase the nodes and increase the number of hidden layers in attempt to push the envelope on accuracy
"""

for nodes_per_layer in [32, 512]:
    
    for layers in [3, 5]:
        mod = create_depth([nodes_per_layer] * layers)
        print(f'Model with{nodes_per_layer: 1} nodes')
        print(f'With{layers: 1} hidden layers')
        eval(mod, epochs=15)
        print()

"""### Wow!  That really didn't help at all!

## One final change that I would like to do is by decreasing the batch size of each iteration from 128 to 16, with 32 nodes, over 25 epochs and determine if that will many any impact
"""

mod = create_depth([32])
eval(mod, 16, 25)

"""## Test it"""

img = 1500
plt.imshow(x_test[img].reshape(28,28),cmap='Greys')
pred = mod.predict(x_test[img].reshape(1, 28, 28))
print(pred.argmax())

"""## Lets Tweak a 3rd Example With What We Have Learned"""

image_size = 28**2 # 28*28
num_classes = 10 # ten unique digits

def create_model(layer_sizes,activation_func='sigmoid',mode='softmax', mode2=None):
    model = Sequential()
    model.add(Flatten(input_shape=(28, 28)))
    model.add(Dense(units=128, activation=activation_func, input_shape=(image_size,))) # increase from 32 nodes to 128 **
    
    for layer_size in layer_sizes[1:]:
      model.add(Dense(units=num_classes, activation=activation_func))    

    model.add(Dense(units=num_classes, activation=mode))
    
    if mode2 != None:
      model.add(Dense(units=num_classes, activation=mode2))
    return model
    
def eval(model, batch_size=128, epochs=5, opt='sgd', loss_func='categorical_crossentropy'):
    model.compile(optimizer=opt, loss=loss_func, metrics=['accuracy'])
    history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=False)
    loss, accuracy  = model.evaluate(x_test, y_test, verbose=False)

    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['training', 'validation'], loc='best')
    plt.show()

    print(f'Test loss: {loss:.3}')
    print(f'Test accuracy: {accuracy:.3}')

af = 'tanh'
opt = 'sgd'
loss = 'mse'
batch = 128
epoch = 5
mode = 'softmax'

for layers in [1, 2]:
    mod = create_model([32] * layers, af)
    print(f'Activation Function: {af, mode}')
    print(f'Model with{32: 1} nodes')
    print(f'With{layers: 1} hidden layer')
    print(f'Loss function: {loss}')
    print(f'Optimizer: {opt}')
    eval(mod, batch, epoch,opt, loss)
    
    print()

"""### In the previous model, we were able to minimize test loss, but were not able to increase accuracy, which is what we are aiming for.  The model is using:
* The Tanh activation function
* The Mean Squared Error loss function
* The Stochastic Gradient Descent (SGD) Optimizer
* 32 Nodes
* One and two hidden layers

### Next, we'll attempt to change the change the optimizer to Adam and keep all other hyperparameters. We'll also stick with a single hidden layer, since that doesn't seem to be helping in this situation:
"""

af = 'tanh'
opt = 'adam'
loss = 'mse'
batch = 128
epoch = 5
nodes = 32
mode = 'softmax'

mod = create_model([nodes], af)
print(f'Activation Function: {af, mode}')
print(f'Model with{32: 1} nodes')
print(f'Loss function: {loss}')
print(f'Optimizer: {opt}')
eval(mod, batch, epoch,opt, loss)
print()

mod.compile(optimizer=opt, loss=loss, metrics=['accuracy'])
img = 1500
plt.imshow(x_test[img].reshape(28,28),cmap='Greys')
pred = mod.predict(x_test[img].reshape(1, 28, 28))
print(pred.argmax())

"""### The accuracy is much better, and with low test loss! 
> However, we still have a way to go on the accuracy.  I'd like to see it over 95%.
"""

af = 'tanh'
opt = 'adam'
loss = 'mae'
batch = 128
epoch = 5
nodes = 32
mode = 'softmax'

mod = create_model([nodes], af)
print(f'Activation Function: {af, mode}')
print(f'Model with{32: 1} nodes')
print(f'Loss function: {loss}')
print(f'Optimizer: {opt}')
eval(mod, batch, epoch,opt, loss)
print()

"""### It appears there isn't much difference between mean squared error vs mean absolute error.  We'll switch back to MSE, since the loss is so much lower in comparison to the tiny increase in test accuracy, for the next one and increase the node pixels to 512"""

af = 'tanh'
opt = 'adam'
loss = 'mse'
batch = 128
epoch = 5
nodes = 128
mode = 'softmax'

mod = create_model([nodes], af)
print(f'Activation Function: {af, mode}')
print(f'Model with{nodes: 1} nodes')
print(f'Loss function: {loss}')
print(f'Optimizer: {opt}')
eval(mod, batch, epoch,opt, loss)
print()

af = 'tanh'
opt = 'adam'
loss = 'mse'
batch = 128
epoch = 5
nodes = 128
mode = 'sigmoid'

mod = create_model([nodes], af, mode)
print(f'Activation Function: {af, mode}')
print(f'Model with{nodes: 1} nodes')
print(f'Loss function: {loss}')
print(f'Optimizer: {opt}')
eval(mod, batch, epoch,opt, loss)
print()

img = 1501
plt.imshow(x_test[img].reshape(28,28),cmap='Greys')
pred = mod.predict(x_test[img].reshape(1, 28, 28))
print(pred.argmax())

af = 'tanh'
opt = 'rmsprop'
loss = 'categorical_crossentropy'
batch = 128
epoch = 5
nodes = 64
mode = 'sigmoid'
mode2 = 'softmax'

mod = create_model([nodes], af, mode, mode2)
print(f'Activation Function: {af, mode, mode2}')
print(f'Model with{nodes: 1} nodes')
print(f'Loss function: {loss}')
print(f'Optimizer: {opt}')
eval(mod, batch, epoch,opt, loss)
print()

"""### It looks the accuracy isn't going to get much better than this at this time.  However, by looking over our progression, it is easy to see how far we've come just by tweaking different parameters and seeing how the model responds"""



mod.compile(optimizer=opt, loss=loss, metrics=['accuracy'])

img = randint(1, 5000)
plt.imshow(x_test[img].reshape(28,28),cmap='Greys')
pred = mod.predict(x_test[img].reshape(1, 28, 28))
print(pred.argmax())